{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/max2000777/Traitement-de-la-langue-naturelle/blob/main/%5BMIDS%5D_TAL_TP_s%C3%A9ance_3_Analyse_de_sujets_latents_Maxime_BRENNAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copier ce notebook (Fichier>Enregistrer une copie dans Drive) puis travailler sur la copie.\n",
        "=="
      ],
      "metadata": {
        "id": "cZyQrbhObEtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But du TP"
      ],
      "metadata": {
        "id": "mphWcg5CrVdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce TP, nous allons faire de l'inférence de sujets latents.\n",
        "Il s'agit de décrire un corpus de textes à l'aide d'un ensemble de sujets (à déterminer), chaque document étant plus ou moins fortement corrélé avec les différents sujets, et chaque sujet étant plus ou moins fortement corrélé avec les différents mots du vocabulaire.\n",
        "Plus précisement, nous allons utiliser la bibliothèque scikit-learn pour entraîner puis analyser un modèle de type Latent Semantic Analysis (LSA) et un modèle de type Latent Dirichlet Allocation (LDA).\n",
        "\n",
        "Nous utiliserons un jeu de données « réelles », composé de sous-titres français de la série Game of Thrones.\n",
        "La première section du notebook génère aussi un jeu de données artificiel, particulièrement adapté à l'analyse par sujets latents.\n",
        "Ce jeu de données permet de tester le code et de montrer ce que les différents systèmes d'analyse étudiés peuvent donner dans un cas idéal.\n",
        "**Si votre code s'exécute sans erreur mais que, au vu des résultats, vous n'êtes pas sûr·e de son bon fonctionnement, testez-le sur le jeu de données artificiel.**"
      ],
      "metadata": {
        "id": "HUxdKgxyra2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use_controlled_dataset = True\n",
        "use_controlled_dataset = False"
      ],
      "metadata": {
        "id": "Mf7h23DOKU6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation d'un jeu de données artificiel"
      ],
      "metadata": {
        "id": "yIh_qwcFKNsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour vérifier le bon fonctionnement de nos systèmes d'analyse de sujets latents, nous allons utiliser un jeu de données artificiel, composé de textes que nous savons être issus de sujets bien distincts.\n",
        "\n",
        "Les « sujets » utilisés ici sont les entiers de `0` à `n_artificial_topics=15`, ou, de manière équivalente, les lettres de l'alphabet de `\"A\"` (pour `0`) à `\"O\"` (pour `15`).\n",
        "À chaque sujet correspond un petit nombre de mots, tous obtenus par répétition de la lettre associée au sujet (ex : « B » et « BBB » pour le sujet 1/B).\n",
        "Chacun des `n_docs=40` documents est associé à un sujet `main_topic` et est constitué de `doc_length=2000` mots, chacun issu d'un sujet choisi avec une probabilité décroissante de sa distance à `main_topic`.\n",
        "\n",
        "Par exemple, le huitième document, d'identifiant « doc_C_7 », est associé au topic 2/C : il contient beaucoup de mots en C, un peu moins de mots en B et D, un peu moins de mots en A et E, etc.\n",
        "\n",
        "Un système d'analyse de sujet devrait être capable de retrouver au moins approximativement les différents sujets utilisés pour générer ce jeu de données artificiel."
      ],
      "metadata": {
        "id": "--VwH1d5KRo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  import numpy as np\n",
        "\n",
        "  n_docs = 40\n",
        "  doc_length = 2000\n",
        "  n_artificial_topics = 15\n",
        "\n",
        "  import random\n",
        "\n",
        "  dataset = []\n",
        "  for i in range(n_docs):\n",
        "    main_topic = int(i  * n_artificial_topics / n_docs)\n",
        "    print(main_topic, end=\", \", flush=True)\n",
        "\n",
        "    tokens = []\n",
        "    for j in range(doc_length):\n",
        "      # Selects a topic for the word, likely to be main_topic, or very near.\n",
        "      topic = main_topic + int(np.random.randn() * 1.5)\n",
        "      while(topic < 0): topic += n_artificial_topics\n",
        "      topic = topic % n_artificial_topics\n",
        "\n",
        "      #\n",
        "      letter = chr(65 + topic) # 65 corresponds to \"A\".\n",
        "      token_length = np.floor(1 + 8 * np.power(np.random.uniform(), 2)) # Between 1 and 9; more likely to be small than large.\n",
        "      token = letter * int(token_length)\n",
        "\n",
        "      tokens.append(token)\n",
        "\n",
        "    dataset.append({\"str_id\": f\"doc_{chr(65+main_topic)}_{i}\", \"raw_text\": \" \".join(tokens)})\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "VS0GMR7eKhiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  print(dataset[7][\"str_id\"])\n",
        "  print(dataset[7][\"raw_text\"]) # Should be composed mainly of words in C, then words in B or D, then words in A or E, etc.\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "9kN2X9woKj1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  for episode in dataset:\n",
        "    print(episode[\"str_id\"], end=\", \", flush=True)\n",
        "    episode[\"processed_text\"] = episode[\"raw_text\"] # No preprocessing for the artificial dataset.\n",
        "    episode[\"processed_tokens\"] = episode[\"raw_text\"].split()\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "vOk7GR33Kk5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_controlled_dataset):\n",
        "  print(dataset[0][\"str_id\"])\n",
        "  print()\n",
        "  print(dataset[0][\"processed_tokens\"][:20])\n",
        "  print(\"[…]\")\n",
        "  print(dataset[0][\"processed_tokens\"][-20:])\n",
        "else:\n",
        "  print(\"[controlled dataset not in use]\")"
      ],
      "metadata": {
        "id": "YnQzJg7zKl4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation du jeu de données réelles"
      ],
      "metadata": {
        "id": "tHjhBDcBray_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le but de cette section est de charger les dialogues de la série Game of Thrones dans une liste `dataset` contenant un dictionaire par épisode.\n",
        "Chaque épisode sera représenté par un dictionnaire indiquant, entre autres,\n",
        "\n",
        "*   un identifiant (*str_id*) sous la forme « SxxEyy » où xx et yy indiquent respectivement le numéro de saison et d'épisode (dans la saison) de l'épisode ;\n",
        "*   une liste de tokens (*processed_tokens*), qui sera par la suite convertie en vecteur numérique de type sac-de-mots (« *bag-of-words* »)."
      ],
      "metadata": {
        "id": "TsQtUy09t4II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Téléchargement et extraction du jeu de données"
      ],
      "metadata": {
        "id": "khSdT7AAs80v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  import os\n",
        "  import urllib # To download files.\n",
        "  import zipfile # To unzip files.\n",
        "\n",
        "  if(True): # True for French data, False for English data.\n",
        "    zip_url = \"https://moodle.u-paris.fr/mod/resource/view.php?id=1329463\" # fr\n",
        "  else:\n",
        "    zip_url = \"https://moodle.u-paris.fr/mod/resource/view.php?id=1330372\" # en\n",
        "\n",
        "  data_dirname = \"data\" # Name of the directory in which the dataset is/will be.\n",
        "\n",
        "  if(os.path.isdir(data_dirname)):\n",
        "    print(\"Dataset found.\")\n",
        "  else:\n",
        "    # Downloads the dataset.\n",
        "    tmp = urllib.request.urlretrieve(zip_url)\n",
        "    filename = tmp[0]\n",
        "    print(f\"Dataset downloaded to '{filename}'.\")\n",
        "\n",
        "    # Extracts the dataset.\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "      zip_ref.extractall(\".\")\n",
        "    assert os.path.isdir(data_dirname)\n",
        "    print(f\"Dataset extracted to '{data_dirname}'.\")\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "Fp7DmgMEt7Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture du jeu de données"
      ],
      "metadata": {
        "id": "Yyep7TlQs20X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le jeu de données contient un fichier SRT par épisode de la série Game of Thrones, groupés par saison (un sous-dossier par saison).\n",
        "(La syntaxe des fichiers SRT est décrite ici : https://docs.fileformat.com/video/srt/)\n",
        "\n",
        "Cette sous-section crée la liste `dataset` dans laquelle chaque épisode est représenté par un dictionnaire indiquant, pour l'instant, son identifiant (*str_id*) et une chaîne de caractères contenant le contenu des sous-titre de l'épisode (*raw_text*)."
      ],
      "metadata": {
        "id": "WhE38XIWtItA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_8dnQFatT8A"
      },
      "outputs": [],
      "source": [
        "if(not use_controlled_dataset):\n",
        "  dataset = [] # This will be a list of dictionaries.\n",
        "\n",
        "  # file_path: str\n",
        "  # Returns a string.\n",
        "  def read_srt_file(file_path):\n",
        "      lines = []\n",
        "      with open(file_path) as f:\n",
        "          c = True\n",
        "          while(c):\n",
        "              s = f.readline() # This should be the end of the file, an empty line or a number.\n",
        "              if(s == \"\"): c = False # The end of the file has been reached.\n",
        "              if(s.strip() == \"\"): continue # End of the file or empty line.\n",
        "\n",
        "              f.readline() # We can throw away the timing that follows.\n",
        "\n",
        "              # All the next non-empty lines are character lines.\n",
        "              s = f.readline().strip()\n",
        "              while(s != \"\"):\n",
        "                  lines.append(s)\n",
        "                  s = f.readline().strip()\n",
        "\n",
        "      return ' '.join(lines)\n",
        "\n",
        "  data_dirname = \"data\"\n",
        "  for path, dirs, files in os.walk(data_dirname): # Iterates through every subdirectories.\n",
        "      path_parts = path.split(os.path.sep)\n",
        "      if(len(path_parts) == 1): continue # There is no subtitle file in the root directory.\n",
        "      season = path_parts[1]\n",
        "      #print(season)\n",
        "      for file in files:\n",
        "          file_parts = file.split(\".\")\n",
        "          episode = file_parts[0]\n",
        "          str_id = f\"{season}-{episode}\"\n",
        "          print(str_id, end= \", \")\n",
        "\n",
        "          file_path = os.path.join(path, file)\n",
        "          #print(file_path)\n",
        "\n",
        "          dataset.append({\"str_id\": str_id, \"raw_text\": read_srt_file(file_path)})\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  dataset = sorted(dataset, key=(lambda x: x[\"str_id\"])) # Sorts the episode chronologically (via their season and episode number).\n",
        "\n",
        "  for episode in dataset: print(episode[\"str_id\"], end=\", \")\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "lf-4ft1xtY1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  print(f'{dataset[0][\"str_id\"]}:')\n",
        "  print(dataset[0][\"raw_text\"][:200]) # Beginning of the first episode.\n",
        "  print(\"[…]\")\n",
        "  print(dataset[0][\"raw_text\"][-200:]) # End of the first episode.\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "lckUTh3vtmDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-traitement du texte"
      ],
      "metadata": {
        "id": "7P_JWkC4wFW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette sous-section enrichit le dictionnaire représentant chaque épisode avec, notamment, la liste de tokens (*processed_tokens*) qui sera ensuite convertie en vecteur numérique de type sac-de-mots (« *bag-of-words* »).\n",
        "Cette liste est obtenue des sous-titres de l'épisode en plusieurs étapes : (i) normalisation/simplification du texte, (ii) tokenisation, (iii) filtrage des mots vides (« *stop words* », c.-à-d. les mots non pertinents pour le problème car n'ayant que peu de valeur sémantique, comme les articles « le/la/… » ou les conjonctions « mais/ou/et/donc/… »)."
      ],
      "metadata": {
        "id": "r-SXr90uxgwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant définit, si nécessaire, l'ensemble de mots vides à utiliser (`stopwords`)."
      ],
      "metadata": {
        "id": "g-RvawDvu9UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  filter_stopwords = True\n",
        "  #filter_stopwords = False\n",
        "\n",
        "  import nltk\n",
        "\n",
        "  if(filter_stopwords):\n",
        "    try:\n",
        "      print(f\"NLTK stop words: {nltk.corpus.stopwords.words('french')}\") # This might fail if \"stopwords\" is missing.\n",
        "    except:\n",
        "      nltk.download('stopwords')\n",
        "      print(f\"NLTK stop words: {nltk.corpus.stopwords.words('french')}\")\n",
        "\n",
        "    stopwords = set()\n",
        "    stopwords.update(set(nltk.corpus.stopwords.words(\"french\")))\n",
        "\n",
        "    # Additional stop words.\n",
        "    stopwords.update({\"a\", \"si\", \"plus\", \"fait\", \"faire\", \"ça\", \"tout\", \"tous\", \"toute\", \"toutes\", \"ce\", \"celui\", \"ceux\", \"celle\", \"celles\", \"son\", \"sa\", \"ses\", \"leur\", \"leurs\", \"tu\", \"dit\", \"oui\", \"non\", \"si\", \"alors\", \"ne\", \"être\", \"avoir\", \"faut\", \"veux\", \"i\", \"ici\", \"là\", \"où\", \"quand\", \"veut\", \"peut\", \"il\", \"ils\", \"elle\", \"elles\", \"mais\", \"ou\", \"et\", \"donc\", \"car\"})\n",
        "\n",
        "    print(f\"Stop words used: {stopwords}\")\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "28CAx0qA-FrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant s'assure de la disponibilité du modèle de tokenisation à utiliser (`nltk.word_tokenize`)."
      ],
      "metadata": {
        "id": "5qTwiZxs15M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  try:\n",
        "    print(nltk.word_tokenize(\"NLTK tokeniser ready.\")) # This might fail if \"punkt\" is missing.\n",
        "  except:\n",
        "    # Modifications in 2025/01/23.\n",
        "    #nltk.download('punkt') # Necessary to use nltk.word_tokenize.\n",
        "    nltk.download('punkt_tab') # Necessary to use nltk.word_tokenize.\n",
        "\n",
        "    print(nltk.word_tokenize(\"NLTK tokeniser ready.\"))\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "pH2IvRqFu7hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant définit, si nécessaire, le modèle de racinisation (« *stemming* ») à utiliser (`stemmer`).\n"
      ],
      "metadata": {
        "id": "nZ_BPCfz1pSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  stem_words = True\n",
        "  stem_words = False\n",
        "\n",
        "  if(stem_words):\n",
        "    stemmer = nltk.stem.snowball.FrenchStemmer() # https://www.nltk.org/api/nltk.stem.snowball.html\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "WfEpdUsO-upF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant définit la fonction utilisée pour pré-traiter le texte de chaque épisode. C'est elle qui effectue les trois étapes, mentionnées plus haut, de normalisation/simplification, tokenisation, et filtrage.\n",
        "\n",
        "Concernant l'étape de normalisation/simplification, la fonction doit au minimum passer tous les caractères en bas de casse et supprimer la ponctuation.\n",
        "\n",
        "Des opérations supplémentaires sont envisageables, au sein de l'étape de normalisation/simplification, comme après la tokenisation."
      ],
      "metadata": {
        "id": "tMLHVeXu12xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  import re # For regexes. https://docs.python.org/3/library/re.html https://docs.python.org/3/howto/regex.html\n",
        "\n",
        "  # text: str\n",
        "  # Returns a pair composed of (i) a string (the text just before tokenization) and (ii) a list of tokens (i.e. strings).\n",
        "  def preprocess(text):\n",
        "      # (i) Normalisation/simplification step\n",
        "      tmp = text\n",
        "      ## TODO\n",
        "      tmp = re.sub(r'[^\\w\\s]', ' ', tmp) # Replaces any non-alphanumeric character (\\w) and non-whitespace character (\\s) with a space.\n",
        "      tmp = re.sub(r'[0-9]', ' ', tmp) # Replaces any numeric character with a space.\n",
        "      tmp = tmp.lower() # Lower cases the string\n",
        "      #tmp = re.sub(r'\\s{2,}', ' ', tmp) # Replaces any duplicate spaces with a single space.\n",
        "      processed_text = tmp\n",
        "\n",
        "      # (ii) Tokenisation step\n",
        "      ## TODO\n",
        "      tokens = nltk.word_tokenize(processed_text)\n",
        "\n",
        "      # (iii) Filtering step\n",
        "      ## TODO\n",
        "      if(filter_stopwords): tokens = [token for token in tokens if(token not in stopwords)]\n",
        "\n",
        "      # (bonus) Stemming step\n",
        "      if(stem_words): tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "      return (processed_text, tokens)\n",
        "\n",
        "  # Test\n",
        "  print(preprocess(\"Yo Jon Snow, comment ça ?  Plutôt bien et vous ? Comptez jusqu'à 3, s'il vous plaît. 1, 2, 3. Nickel.\"))\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "iOAZ5JLN0bQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant effectue le pré-traitement des épisodes implémenté par la fonction définie ci-dessus."
      ],
      "metadata": {
        "id": "YRYTWaIT5Qmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  for episode in dataset:\n",
        "    print(episode[\"str_id\"], end=\", \", flush=True)\n",
        "\n",
        "    (text, tokens) = preprocess(episode[\"raw_text\"])\n",
        "    episode[\"processed_text\"] = text\n",
        "    episode[\"processed_tokens\"] = tokens\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "5V7sA92f0YKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(not use_controlled_dataset):\n",
        "  print(dataset[0][\"str_id\"])\n",
        "  print()\n",
        "  print(dataset[0][\"processed_tokens\"][:20])\n",
        "  print(\"[…]\")\n",
        "  print(dataset[0][\"processed_tokens\"][-20:])\n",
        "else:\n",
        "  print(\"[controlled dataset in use]\")"
      ],
      "metadata": {
        "id": "nz1R6bvOu_CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0].keys())"
      ],
      "metadata": {
        "id": "d5UCsFodcgwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création de la matrice de comptage"
      ],
      "metadata": {
        "id": "y0YSTj4y2iR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le but de cette section est de créer une matrice de comptage `matrix` indiquant, pour un ensemble de formes choisies (c.-à-d. type de tokens), pour chaque document, le nombre d'occurrences de chaque forme dans le document.\n",
        "\n",
        "Nous allons procéder en trois étapes : (i) comptage par document des formes, (ii) création d'un vocabulaire incluant ou non un forme en fonction de sa fréquence de documents (c.-à-d. la proportion d'épisodes dans lesquels elle apparaît ; le but étant de filtrer les mots trop rares et les mots trop fréquents), (iii) création de la matrice de comptage, restreinte au vocabulaire fixé."
      ],
      "metadata": {
        "id": "I-jUwVvS66lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comptage par document des formes"
      ],
      "metadata": {
        "id": "_7GW6obzT2cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le bloc suivant compte (i) pour chaque forme, le nombre de documents dans laquelle est apparaît, et (ii) pour chaque document, le nombre d'occurrences de chaque forme."
      ],
      "metadata": {
        "id": "F9CyZO_PvBYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter # https://docs.python.org/3/library/collections.html#collections.Counter\n",
        "\n",
        "document_form_counts = Counter() # Form each form, the number of documents it occurs in.\n",
        "for episode in dataset:\n",
        "    print(episode[\"str_id\"], end=\", \", flush=True)\n",
        "\n",
        "    episode[\"counts\"] = Counter(episode[\"processed_tokens\"]) # For each form, the number of its occurences in the document.\n",
        "    document_form_counts.update(set(episode[\"processed_tokens\"]))\n",
        "print()\n",
        "\n",
        "print(document_form_counts)"
      ],
      "metadata": {
        "id": "SwnNzst-vTJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0].keys())"
      ],
      "metadata": {
        "id": "ADGz-qYaeUiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création d'un vocabulaire"
      ],
      "metadata": {
        "id": "r82thsNYSNBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit ici de créer le vocabulaire utilisé par la suite, contenant les formes dont la fréquence de document est supérieure à une limite inférieure `min_df` et inférieure à une limite supérieure `max_df`.\n",
        "Ce vocabulaire se présentera sous la forme d'un dictionnaire `form2id`, associant à chaque forme un identifiant entier, et, inversement, d'une liste `id2form`, associant à chaque entier la forme correspondante."
      ],
      "metadata": {
        "id": "nk97M4z1NxY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_df = 0.9 # Upper limit for the document frequency of a form.\n",
        "min_df = 0.06 # Lower limit for the document frequency of a form.\n",
        "\n",
        "# TODO\n",
        "form2id = {} # From form (str) to id (int).\n",
        "id2form = [] # From id (int) to form (str).\n",
        "Nb_doc=len(dataset)\n",
        "\n",
        "for forme, comptage in document_form_counts.items():\n",
        "  episode_frequence= comptage/Nb_doc\n",
        "  if episode_frequence<=max_df and episode_frequence>=min_df :\n",
        "    form2id[forme]=len(form2id)\n",
        "    id2form.append(forme)\n",
        "\n",
        "\n",
        "print(f\"Number of forms: {len(id2form)}\")"
      ],
      "metadata": {
        "id": "UiW5QRYC6oaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(form2id)"
      ],
      "metadata": {
        "id": "h30EizAFge8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création de la matrice de comptage"
      ],
      "metadata": {
        "id": "iEiz3ng4SP8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créer une matrice de comptage sous forme d'un tableau Numpy bidimensionnel.\n",
        "Les lignes doivent être indicées par les documents, les colonnes par les formes."
      ],
      "metadata": {
        "id": "dcX6mmF5Qnlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO\n",
        "matrix = np.zeros((Nb_doc,len(form2id)))\n",
        "print(matrix.shape)\n",
        "for i  in range(0,len(dataset)):\n",
        "  for forme,comptage in dataset[i]['counts'].items():\n",
        "    if forme in id2form:\n",
        "      matrix[i,form2id[forme]]=comptage\n",
        "\n",
        "print(matrix)\n",
        "\n",
        "doc_lengths = matrix.sum(axis=1) # For each document, its length.\n",
        "term_frequency = matrix.sum(axis=0) # For each form, its frequency (count)."
      ],
      "metadata": {
        "id": "LCYtrtVx8rKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "kTYJEIGSOQdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entraînement d'un modèle"
      ],
      "metadata": {
        "id": "-oiXuufma7nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD # https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "\n",
        "lsa_n_topics = 15\n",
        "assert lsa_n_topics <= len(dataset) # With LSA, their cannot be more topics than documents.\n",
        "lsa_model = TruncatedSVD(n_components=lsa_n_topics, n_iter=10)\n",
        "\n",
        "print(\"Fitting the model…\", end=\"\", flush=True)\n",
        "lsa_model.fit(X=matrix)\n",
        "print(\" Done!\")"
      ],
      "metadata": {
        "id": "MNfv7cdIa9--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse des paramètres"
      ],
      "metadata": {
        "id": "2yYybUKrnYsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_form_corr = lsa_model.components_ # The V^t matrix. Contains a description of each topic in terms of the forms (positive/negative values are interpreted as positive/negative correlations).\n",
        "print(topic_form_corr.shape)\n",
        "print(topic_form_corr)"
      ],
      "metadata": {
        "id": "EKz5LK28dYR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_topic_corr = lsa_model.transform(matrix) # (U * Σ). Contains a description of each document in terms of the topics (positive/negative values are interpreted as positive/negative correlations).\n",
        "print(document_topic_corr.shape)\n",
        "print(document_topic_corr)"
      ],
      "metadata": {
        "id": "VBn2cwuoce3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit ici, pour chaque sujet, de retrouver les formes de poids maximal et les formes de poids minimal."
      ],
      "metadata": {
        "id": "uWS82y4bOnfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_forms = 10\n",
        "def lsa_show_topic(topic_id):\n",
        "    print(f\"Topic n°{topic_id}:\")\n",
        "\n",
        "    topic_vector = topic_form_corr[topic_id]\n",
        "    #print(len(topic_vector)) # From id (int) to score (float).\n",
        "\n",
        "    # TODO\n",
        "    positive_corr_id = []\n",
        "    negative_corr_id = []\n",
        "    for i in range(0,len(topic_vector)):\n",
        "      if topic_vector[i]>0:\n",
        "        positive_corr_id.append(i)\n",
        "      else:\n",
        "        negative_corr_id.append(i)\n",
        "    print(f\"positive correlation: {[(id2form[i], topic_vector[i]) for i in positive_corr_id]}\")\n",
        "    print(f\"negative correlation: {[(id2form[i], topic_vector[i]) for i in negative_corr_id]}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "for topic_id in range(lsa_n_topics): lsa_show_topic(topic_id) # The first topics are the most important."
      ],
      "metadata": {
        "id": "z555oYO2kFZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Dirichlet Allocation"
      ],
      "metadata": {
        "id": "b0WzcoVPOcAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entraînement d'un modèle"
      ],
      "metadata": {
        "id": "duTX1XJL56pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda_n_topics = 15\n",
        "lda_model = LatentDirichletAllocation(n_components=lda_n_topics, max_iter=20, n_jobs=-1)\n",
        "\n",
        "print(\"Fitting the model…\", end=\"\", flush=True)\n",
        "lda_model.fit(X=matrix)\n",
        "print(\" Done!\")\n",
        "\n",
        "topic_term_dists = lda_model.components_ / np.expand_dims(lda_model.components_.sum(axis=1), axis=-1) # Contains, for each topic, the probability distribution of generation over forms.\n",
        "doc_topic_dists = lda_model.transform(matrix) # Contains, for each document, the probability distribution over topics."
      ],
      "metadata": {
        "id": "LgLaoF74vZqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse des paramètres"
      ],
      "metadata": {
        "id": "xCT3Kyh56Bby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit ici, pour chaque sujet, de retrouver les formes de probabilité maximale.\n",
        "\n",
        "Noter qu'il est possible que certains sujets soient très « étalés », c'est-à-dire aient une distribution assez uniforme sur le vocabulaire.\n",
        "Ces sujets ne sont pas très informatifs et on pourra vouloir les ignorer par la suite."
      ],
      "metadata": {
        "id": "KgnpgK2WO7KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_forms = 20\n",
        "def lda_show_topic(topic_id):\n",
        "    print(f\"Topic n°{topic_id}:\")\n",
        "\n",
        "    topic_vector = topic_term_dists[topic_id]\n",
        "    print(topic_vector) # From id (int) to score (float).\n",
        "\n",
        "    # TODO\n",
        "    significant_id = ()\n",
        "\n",
        "    print([(id2form[i], topic_vector[i]) for i in significant_id])\n",
        "\n",
        "    print()\n",
        "\n",
        "for topic_id in range(lda_n_topics): lda_show_topic(topic_id)"
      ],
      "metadata": {
        "id": "ptMS8-o_vawo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On regarde ici, pour chaque document, les principaux sujets qui lui sont associés (c.-à-d., qui, d'après le modèle, interviennent dans la génération du document).\n",
        "L'ensemble, sur tous les documents, des sujets principaux, est calculé dans `major_topics`."
      ],
      "metadata": {
        "id": "JO7JxjiEPNWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "major_topics = set()\n",
        "\n",
        "for (document_id, document_vector) in enumerate(doc_topic_dists):\n",
        "    #print(f\"Document n°{document_id}:\")\n",
        "    print(f\"{dataset[document_id]['str_id']}:\")\n",
        "\n",
        "    #print(document_vector) # From id (int) to score (float).\n",
        "\n",
        "    sorted_id = document_vector.argsort() # https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "    sorted_id = np.flip(sorted_id) # https://numpy.org/doc/stable/reference/generated/numpy.flip.html\n",
        "    major_id = sorted_id[:n_forms]\n",
        "    major_id = [i for i in major_id if(document_vector[i] > (0.1 * document_vector.max()))]\n",
        "    #major_id = [i for i in major_id if(document_vector[i] > (1.1 * document_vector.min()))]\n",
        "\n",
        "    print(major_id)\n",
        "    print([document_vector[i] for i in major_id]) # Prints the probability within this document of each major topic.\n",
        "    major_topics.update(major_id)\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "72sjHJFivdvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(major_topics)\n",
        "print()\n",
        "\n",
        "for topic_id in major_topics: lda_show_topic(topic_id)"
      ],
      "metadata": {
        "id": "kNIo042JvoCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe ici la présence de chaque sujet de `major_topics` dans les différents documents du jeu de données."
      ],
      "metadata": {
        "id": "UEnZU82jPpL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "\n",
        "time = np.arange(len(dataset))\n",
        "for i, topic_evolution in enumerate(np.transpose(lda_model.transform(matrix))):\n",
        "    if(i not in major_topics): continue\n",
        "    ax.plot(time, topic_evolution, label=f\"topic {i}\")\n",
        "\n",
        "ax.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZiYiSTPDvtZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On représente ici le modèle LDA, en particulier les sujets inférés, à l'aide de la bibliothèque pyLDAvis (https://pyldavis.readthedocs.io/en/latest/readme.html).\n",
        "\n",
        "L'interface est essentiellement composée d'un panneau gauche, organisant spatialement les différents sujets, et d'un panneau droit représentant la distribution sur le vocabulaire d'un sujet particulier.\n",
        "Le panneau droit montre, pour un sujet sélectionné, les formes qui lui sont le plus « pertinentes ».\n",
        "La notion de pertinence utilisée dépend d'un paramètre λ réglable et est décrite en bas du panneau.\n",
        "Avec λ=1, les mots les plus pertinents sont ceux qui sont les plus fréquemment générés par ce sujet.\n",
        "Avec λ=0, les mots les plus pertinents sont ceux qui sont les plus spécifiques à ce sujet.\n",
        "\n",
        "Noter que la numération des sujets par cet outil n'est pas celle utilisée plus haut.\n",
        "L'outil organise les sujets par importance décroissante dans le corpus, l'importance d'un sujet étant le nombre de tokens qu'il a généré (remarque : se rappeler le modèle génératif de la LDA)."
      ],
      "metadata": {
        "id": "4MP6NpBSP7BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import pyLDAvis\n",
        "except:\n",
        "  !pip install pyLDAvis\n",
        "  #!pip install pyLDAvis==3.3.1\n",
        "  import pyLDAvis\n",
        "\n",
        "vis_data = pyLDAvis.prepare(topic_term_dists=topic_term_dists, doc_topic_dists=doc_topic_dists, doc_lengths=doc_lengths, vocab=id2form, term_frequency=term_frequency, mds='mmds') # https://pyldavis.readthedocs.io/en/latest/modules/API.html#pyLDAvis.prepare\n",
        "pyLDAvis.display(vis_data) # Warning: This tool numbers the topics according to their importance in the corpus (in term of number of tokens). It is possible to recognise a topic by looking at the list of forms displayed for λ=1. The list displayed for λ=0 shows the forms that are the most specific to the selected topic."
      ],
      "metadata": {
        "id": "fBR3mQdovv-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top2Vec"
      ],
      "metadata": {
        "id": "qk9aldHiLS2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from top2vec import Top2Vec\n",
        "except:\n",
        "  !pip install top2vec\n",
        "  from top2vec import Top2Vec"
      ],
      "metadata": {
        "id": "0X6Bgb5BLVlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top2Vec ne fonctionne pas bien (le code peut même crasher) sur des petits ensembles de documents. C'est pourquoi le code suivant scinde chaque document en parties de moins de `max_tokens=500` tokens."
      ],
      "metadata": {
        "id": "tNk_2666LW4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "max_tokens = 250\n",
        "documents = [\" \".join(tokens) for tokens in itertools.chain(*[np.array_split(episode[\"processed_tokens\"], np.ceil(len(episode[\"processed_tokens\"])/max_tokens)) for episode in dataset])] # list[str]\n",
        "\n",
        "print(f\"from {len(dataset)} to {len(documents)} (shorter) documents\")\n",
        "print()\n",
        "\n",
        "ids = [0, (len(documents)//2), -1]\n",
        "for i in ids:\n",
        "  print(f\"Document n°{i}:\")\n",
        "  print(documents[i])\n",
        "  print(f\"(char len: {len(documents[i])})\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "LzyungQpLYSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entraînement du modèle"
      ],
      "metadata": {
        "id": "cOKqpJSbLcH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torch\n",
        "  gpu_available = (torch.cuda.device_count() > 0)\n",
        "except:\n",
        "  gpu_available = False\n",
        "\n",
        "if(not gpu_available): print(\"No GPU available. Neural computations might be very slow. You might be able to make a GPU available by changing the notebook's settings.\")\n",
        "else: print(\"GPU available.\")"
      ],
      "metadata": {
        "id": "UpZnkC0HLdrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top2vec_model = Top2Vec(documents=documents, ngram_vocab=False, contextual_top2vec=False) # https://top2vec.readthedocs.io/en/latest/api.html#top2vec.top2vec.Top2Vec"
      ],
      "metadata": {
        "id": "axwXJ53XLfTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse"
      ],
      "metadata": {
        "id": "zlujK8dkLhes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Garder à l'esprit que Top2Vec tend à ne donner des résultats intéressants que quand les sujets ne sont pas trop mélangés au sein de chaque document. Ça marche par exemple très bien avec le jeu de données artificiel."
      ],
      "metadata": {
        "id": "VfMTtJ0YLlG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = top2vec_model.get_num_topics()\n",
        "print(f\"{num_topics} topics induced.\")\n",
        "\n",
        "print()\n",
        "topic_words, word_scores, topic_nums = top2vec_model.get_topics(num_topics)\n",
        "for i in topic_nums:\n",
        "  print(\"Top 50 vocabulary item in terms of semantic similarity to the topic n°{i}:\")\n",
        "  print(list(zip(topic_words[i], word_scores[i])))"
      ],
      "metadata": {
        "id": "RS_AOuPqLjSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in topic_nums:\n",
        "  top2vec_model.generate_topic_wordcloud(i)"
      ],
      "metadata": {
        "id": "Eg5dje6BLzZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "La bibliothèque Top2Vec propose d'autres fonctionnalités.\n",
        "Plus d'information :\n",
        "*  sur la page GitHub de Top2Vec, https://github.com/ddangelov/Top2Vec\n",
        "*  dans la documentation de Top2Vec, https://top2vec.readthedocs.io\n",
        "\n",
        "Voir aussi les (pré)publications scientifiques :\n",
        "*   Angelov, Dimo. « Top2Vec: Distributed Representations of Topics ». arXiv, 19 août 2020. https://doi.org/10.48550/arXiv.2008.09470.\n",
        "*   Angelov, Dimo, et Diana Inkpen. « Topic Modeling: Contextual Token Embeddings Are All You Need ». In Findings of the Association for Computational Linguistics: EMNLP 2024, édité par Yaser Al-Onaizan, Mohit Bansal, et Yun-Nung Chen, 13528‑39. Miami, Florida, USA: Association for Computational Linguistics, 2024. https://doi.org/10.18653/v1/2024.findings-emnlp.790.\n"
      ],
      "metadata": {
        "id": "Fx2hq7qcL3vR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}